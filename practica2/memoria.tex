\documentclass[oneside]{book}

\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry} 

\usepackage[utf8]{inputenc}   % otra alternativa para los caracteres acentuados y la "Ã±"
\usepackage[           spanish % para poder usar el espaÃ±ol
                      ,es-tabla % para los captions de las tablas
                       ]{babel}   
\decimalpoint %para usar el punto decimal en vez de coma para los nÃºmeros con decimales

%\usepackage{beton}
%\usepackage[T1]{fontenc}

\usepackage{parskip}
\usepackage{xcolor}

\usepackage{caption}

\usepackage{enumerate} % paquete para poder personalizar fÃ¡cilmente la apariencia de las listas enumerativas

\usepackage{graphicx} % figuras
\usepackage{subfigure} % subfiguras

\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{listings}
\lstset
{ %Formatting for code in appendix
    language=python,
    basicstyle=\footnotesize,
    stepnumber=1,
    showstringspaces=false,
    tabsize=1,
    breaklines=true,
    breakatwhitespace=false,
}

\definecolor{gris}{RGB}{220,220,220}
	
\usepackage{float} % para controlar la situaciÃ³n de los entornos flotantes

\restylefloat{figure}
\restylefloat{table} 
\setlength{\parindent}{0mm}


\usepackage[bookmarks=true,
            bookmarksnumbered=false, % true means bookmarks in 
                                     % left window are numbered
            bookmarksopen=false,     % true means only level 1
                                     % are displayed.
            colorlinks=true,
            allcolors=blue,
            urlcolor=blue]{hyperref}
\definecolor{webblue}{rgb}{0, 0, 0.5}  % less intense blue

\renewcommand{\thesection}{\arabic{section}}

\title{\Huge Inteligencia de Negocio: Práctica 2 \\ Visualización y Segmentación\vspace{10mm}}

\author{\huge David Cabezas Berrido \vspace{10mm} \\ 
  \huge Grupo 2: Viernes \vspace{10mm} \\ \huge dxabezas@correo.ugr.es \vspace{10mm}}

\begin{document}
\maketitle
\tableofcontents
\begin{center}
\vspace*{8cm}
\part{\textbf{Visualización}}
\end{center}

Sobre los resultados de la práctica 1, realizaremos representaciones
de los resultados obtenidos por cada algoritmo y preprocesamiento,
también de las relaciones entre los atributos. No nos centraremos en
por qué unos modelos o preprocesamientos son mejores que otros, ni en
detalles de los mismos (hiperparámetros configurados o por
defecto. Eso ya lo hicimos en la práctica anterior. Nos centraremos
cómo podemos interpretar las visualizaciones para ayudarnos a
distinguir qué algoritmo o procesamiento es más adecuado en cada caso,
o qué relaciones hay entre los atributos.

\section{Visualización de medidas}
Compararemos los scores de todos los modelos para cada
preprocesamiento y viceversa. Las métricas que representaremos son la
Accuracy y el F1-score.

\subsection{Por procesamiento}

El \textbf{primer procesamiento} consiste en \textbf{eliminar las
  instancias con valores perdidos}.

\begin{figure}[H]
  \centering
  \caption{Desempeño de los algoritmos sobre el preprocesado 1: Eliminación de valores perdidos}
  \label{fig:dropna}
  \includegraphics[width=180mm]{figures/visualizacion/dropna}
\end{figure}

La barra azul (Accuracy) representa el desempeño general del modelo,
la proporción de instancias bien clasificadas en validación
cruzada. Mientras que la barra naranja (F1-score) le da una mayor
prioridad a la clasificación de instancias positivas, que tienen mayor
importancia en este problema, puesto que un falso negativo es más
grave que un falso positivo.

Los modelos que más score han obtenido para este preprocesamiento son
el Árbol de Decisión y Naive-Bayes. El Árbol de Decisión tiene
ligeramente mayor Accuracy, mientras que Naive-Bayes tiene más
F1-score, esto significa que Decision Tree clasifica mejor las
instancias negativas que Naive-Bayes, y éste último clasifica mejor
las positivas.

SVM presenta resultados bastante peores al resto de algoritmos
(obviando Dummy).

Generalmente, la Accuracy y la F1-score son similares. La excepción es
Dummy, que clasifica bien todas las positivas pero ninguna negativa.

El \textbf{segundo procesamiento} consiste en \textbf{imputar los
  valores perdidos}, para las variables numéricas usamos la mediana y
para las nominales, la moda.

\begin{figure}[H]
  \centering
  \caption{Desempeño de los algoritmos sobre el preprocesado 2: Imputación de valores perdidos}
  \label{fig:median}
  \includegraphics[width=170mm]{figures/visualizacion/median}
\end{figure}

La comparación entre los modelos no ha cambiado mucho, salvo un
descenso en el desempeño de todos los modelos. La gráfica nos ayuda a
observar, comparando las barras azules con las naranjas, que el
descenso ha sido mayor en la F1-score (la barra naranja ahora está más
abajo que la azul en todos los algoritmos salvo Dummy), esto significa
que, en general, ha empeorado la clasificación de instancias
positivas.

El \textbf{tercer procesamiento} consiste en \textbf{simplificar el
  conjunto de atributos}. Concretamente, eliminamos Density y
simplificamos la distribución de BI-Rads para que sólo tome los
valores 4 y 5.

\begin{figure}[H]
  \centering
  \label{fig:features}
  \caption{Desempeño de los algoritmos sobre el preprocesado 3: Eliminación de características innecesarias}
  \includegraphics[width=170mm]{figures/visualizacion/features}
\end{figure}

Con este preprocesamiento, observamos que se ha recuperado la
similitud entre los valores de F1-score y Accuracy, puesto que se hizo
sobre los datos del preprocesado 1 y no sobre los del 2. También se ha
producido una mejora general en la mayoría de modelos, sobre todo en
Naive-Bayes por lo que comentamos en la práctica anterior de que
supone los atributos independientes. Sin embargo, la comparación entre
los modelos no cambia demasiado.

El \textbf{cuarto procesamiento} consiste en \textbf{binarizar las
  características nominales}, Shape y Margin. Esto lo hicimos porque
no tenía sentido medir la distancia entre valores de una variable
nominal si los numerábamos con números naturales, y algoritmos que no
tratan bien las variables nominales como KNN y Neural Network podían
verse afectados por esto.

\begin{figure}[H]
  \centering
  \label{fig:binarization}
  \caption{Desempeño de los algoritmos sobre el preprocesado 4: Binarización de características nominales}
  \includegraphics[width=180mm]{figures/visualizacion/binarization}
\end{figure}

Se aprecia un descenso en Naive-Bayes, por haber introducido nuevas
características que no son independientes unas de otras (cuando una
vale 1, el resto de características binarizas asociadas a la misma
variable nominal están forzadas a valer 0). La mejora en KNN es muy
leve, pero sí destaca ahora una subida en la colummna de Neural
Network.

Este procesamiento también perjudica aún más a SVM, aumenta más la
diferencia entre su columna y el resto.

El \textbf{quinto procesamiento} consiste en \textbf{reescalar las
  variables}, para que tengan media 0 y varianza 1. Esto se hizo para
solucionar la importancia exagerada que cobraba la variable Age en
algoritmos como KNN.

\begin{figure}[H]
  \centering
  \label{fig:stdScaler}
  \caption{Desempeño de los algoritmos sobre el preprocesado 5: Estandarizado de los datos}
  \includegraphics[width=180mm]{figures/visualizacion/stdScaler}
\end{figure}

Lo que más descata en esta gráfica es la importante subida de SVM, su
columna sube hasta acabar ligeramente por encima del resto. También
observamos una subida notable en el desempeño de KNN, cuya barra
adelanta a la de Random Forest.

\subsection{Por modelo}

\begin{figure}[H]
  \centering
  \label{fig:dummy}
  \caption{Desempeño de Dummy para cada preprocesamiento}
  \includegraphics[width=90mm]{figures/visualizacion/dummy}
\end{figure}

No hay mucho que comentar de Dummy, consigue los mismos scores para
todos los preprocesamientos, ya que ignora los datos de entrada y se
limita a predecir siempre maligno. Cambia para el el segundo
preprocesamiento, porque varía el número de instancias, ya que no
eliminamos las instancias con valores perdidos.

\begin{figure}[H]
  \centering
  \label{fig:dt}
  \caption{Desempeño de Decision Tree para cada preprocesamiento}
  \includegraphics[width=90mm]{figures/visualizacion/decisionTree}
\end{figure}

La eficacia de Decision Tree no se vió afectada apenas por ninguno de
los procesamientos que planteamos. Trabaja con las variables por
separado, por lo que no le afecta el reescalado; trabaja bien con las
variables nominales, por lo que no se ve beneficiado de la
binarización de las mismas. Sí que observamos que este modelo
acostumbra a clasificar mejor las instancias negativas, ya que su
desempeño general (barra azul) está por encima de su F1-score (barra
naranja), que da mayor importancia a los ejemplos positivos.

\begin{figure}[H]
  \centering
  \label{fig:nb}
  \caption{Desempeño de Naive-Bayes para cada preprocesamiento}
  \includegraphics[width=90mm]{figures/visualizacion/gaussianNB}
\end{figure}

Sobre este modelo percibimos que funciona mejor tras la simplificación
del conjunto de características, pero empeora al introducir
características nuevas que no son independientes cuando binarizamos
las características nominales. Sobre algunos preprocesamientos ha
tenido similar desempeño al clasificar instancias positivas y
negativas (las barras azul y naranja están a la misma altura). Al
igual que el resto de modelos, al imputar los valores perdidos pierde
bastante efectividad sobre los ejemplos positivos (la barra naranja
está más abajo que la azul). Al introducir las características
dependientes en la binarización de variables nominales, decrece su
eficacia sobre todo para los ejemplos negativos (la barra naranja no
decrece tanto como la azul), y tampoco se ve afectado por el
reescalado de los datos.

\begin{figure}[H]
  \centering
  \caption{Desempeño de SVM para cada preprocesamiento}
  \label{fig:svm}
  \includegraphics[width=100mm]{figures/visualizacion/svm}
\end{figure}

SVM presenta un desempeño bastante pobre hasta llegar al reescalado de
las variables, cuando se convierte en el modelo que más score
consigue. Se ve perjudicado por la binarización de características
nominales y, exceptuando lo que les ocurre a todos los algoritmos con
el segundo preprocesado (imputar valores perdidos), su desempeño es
similar en las características positivas y en las negativas (las
barras azul y naranja están muy igualadas).

\begin{figure}[H]
  \centering
  \caption{Desempeño de Random Forest para cada preprocesamiento}
  \label{fig:rf}
  \includegraphics[width=100mm]{figures/visualizacion/randomForest}
\end{figure}

A Random Forest le ocurre lo mismo que a Decision Tree (Random Forest
es un modelo que promedia varios Decision Tree), aunque su desempeño
en general es peor. Hay un poco menos de diferencia entre las barras
azul y naranja, por lo que no empeora tanto al clasificar instancias
positivas.

\begin{figure}[H]
  \centering
  \caption{Desempeño de KNN para cada preprocesamiento}
  \label{fig:knn}
  \includegraphics[width=100mm]{figures/visualizacion/knn}
\end{figure}

KNN se ve beneficiado por los dos últimos procesamientos, sobre todo
con el último, el reescalado de las variables. Presenta resultados
similares para todos los procesamientos, en comparación con los otros
modelos, empeora poco con la imputación de valores perdidos. Su
desempeño es similar para todos los preprocesamientos excepto para el
último, donde la mejora es notable.

\begin{figure}[H]
  \centering
  \caption{Desempeño de Neural Network para cada preprocesamiento}
  \label{fig:neuralNetwork}
  \includegraphics[width=100mm]{figures/visualizacion/neuralNetwork}
\end{figure}

Neural Network se ve perjudicado por la simplificación del conjunto de
características, probablemente porque es un modelo con una alta
complejidad y capacidad explicativa y sea capaz de aprovechas la poca
información que se pierde con esta simplificación. Los preprocesados
de binarización y estandarización le benefician en gran medida. En
general, su eficacia es similar en ejemplos positivos y negativos, las barras azul y naranja están prácticamente a la misma altura.

\section{Gráficas de la curva ROC}

Para cada preprocesamiento, compararemos los modelos representándolos
en el espacio ROC. Los estamos tratando como modelos discretos (sólo
atendemos a la clase que predigan, no la probabilidad que asignen de
pertenencia a cada clase), por lo que no vamos a dibujar sus curva ROC
como es habitual verlas, con forma de codo. Esto podríamos hacerlo con
la función \texttt{roc\_curve} de \texttt{sklearn.metrics}, pero
tenemos que tener claro la interpretación continua que se hace del
modelo para interpretar las curvas. Por ejemplo un árbol de decisión
se puede interpretar como un modelo continuo de la siguiente manera:
si una instancia cae sobre una hoja con $p$ instancias de
entrenamiento de la clase maligno y $q$ de la clase benigno, puede
asignar una probabilidad de $\frac{p}{p+q}$ para la clase maligno y
una de $\frac{q}{p+q}$ para la clase benigno.

En su lugar, cada modelo aparecerá como un punto en el plano,
concretamente en el intervalo $[0,1]\times[0,1]$.

El análisis gráfico por gráfico sería muy repetitivo, por lo que nos
limitaremos a explicar cómo se interpretan estos gráficos. Lo haremos
sobre el primero de ellos.

\begin{figure}[H]
  \centering
  \caption{Algoritmos sobre el preprocesado 1 representados en el espacio ROC}
  \includegraphics[width=120mm]{figures/visualizacion/roc1}
  \label{fig:roc1}
\end{figure}

En el eje horizontal se representa el FPR, la tasa de falsos positivos
(número de predicciones positivas incorrectas entre el número total de
ejemplos negativos). En el eje vertical se representa el TPR, la tasa
de verdaderos positivos (número de predicciones positivas correctas
entre el número total de ejemplos positivos).

Cuanto más a la izquierda esté un punto, menor será la FPR del modelo,
luego mejor clasificará ejemplos negativos. Cuanto más a arriba esté
un punto, mayor será la TPR del modelo, luego mejor clasificará
ejemplos positivos.

Si un modelo está situado arriba a la derecha de otro, significa que
predice mejor tanto los ejemplos positivos como los negativos, por lo
que podemos considerarlo mejor en cuanto a resultados, sin tener en
cuenta la simplicidad ni la facilidad de interpretación de los
modelos. De otra forma, dos modelos no son tan fácilmente comparables,
por lo que tendríamos que atender a sus diferencias de FPR y TPR, a la
importancia que le demos a cada una.

En este ejemplo concreto (Figura \ref{fig:roc1}), Naive-Bayes es mejor
que Neural Network, KNN y SVM. Obviamente Dummy supera a todos a la
hora de clasificar ejemplos positivos, pero es el peor clasificando
negativos. Naive-Bayes supera al árbol de decisión a la hora de
clasificar instancias positivas, pero es peor clasificando instancias
negativas. También apreciamos que SVM es el peor de todos sin contar
Dummy.

A continuación están las representaciones de los modelos en el espacio
ROC para el resto de preprocesamientos.

\begin{figure}[H]
  \centering
  \subfigure[Preprocesado 2: Imputación]{
    \includegraphics[width=85mm]{figures/visualizacion/roc2}
  }
  \subfigure[Preprocesado 3: Características]{
    \includegraphics[width=85mm]{figures/visualizacion/roc3}
  }
  \subfigure[Preprocesado 4: Binarización]{
    \includegraphics[width=85mm]{figures/visualizacion/roc4}
  }
  \subfigure[Preprocesado 5: Estandarización]{
    \includegraphics[width=85mm]{figures/visualizacion/roc4}
  }
  \caption{Algoritmos sobre cada preprocesado representados en el espacio ROC}
\end{figure}

\newpage

\section{Análisis de atributos}

Para cada atributo, representamos para cada posible valor el número de
instancias positivas (en azul) y negativas (en naranja). Utilizamos un
gráfico de barras apiladas, con el número de instancias malignas
abajo. En este tipo de gráficos será fácil saber cuantas instancias
malignas tienen determinado valor de una variable, y también cuantas
instancias totales. Para ver el número de instancias benignas habrá
que tener en cuenta donde empieza la barra.

\begin{figure}[H]
  \centering
  \caption{Distribución de ejemplos de cada clase para los distintos valores de BI-RADS}
  \includegraphics[width=140mm]{figures/visualizacion/birads}
  \label{fig:birads}
\end{figure}

La mayoría de instancias tienen los valores 4 y 5. Al aumentar el
valor de la variable, aumenta considerablemente la proporción de
ejemplos malignos con ese valor. Por tanto, vemos que esta variable
está altamente relacionada con la severidad del tumor.

\begin{figure}[H]
  \centering
  \caption{Distribución de ejemplos de cada clase para los distintos valores de Age}
  \includegraphics[width=120mm]{figures/visualizacion/age}
  \label{fig:age}
\end{figure}

Al haber tantos valores es lógico que halla impurezas, pero al igual
que antes, observamos un aumento en la proporción de ejemplos malignos
para valores altos de la variable. Por lo que también concluimos que
la edad está bastante relacionada con la severidad.

\begin{figure}[H]
  \centering
  \caption{Distribución de ejemplos de cada clase para los distintos valores de Density}
  \includegraphics[width=120mm]{figures/visualizacion/density}
  \label{fig:density}
\end{figure}

Observamos que la mayoría de las instancias presentan el valor 3 de
esta variable. Además, para cada valor el número de instancias
positivas y negativas que lo presentan es prácticamente el mismo. Por
tanto, no parece que esta variable influya en la severidad del tumor.

\begin{figure}[H]
  \centering
  \caption{Distribución de ejemplos de cada clase para los distintos valores de Margin}
  \includegraphics[width=115mm]{figures/visualizacion/margin}
  \label{fig:margin}
\end{figure}

La mayoría de las instancias que presentan el valor 1 corresponden a
tumores benignos y la mayortía de las que presentan el valor 5, a
malignos. Para el resto de valores (2, 3 y 4), la proporción de
instancias malignas es mayor que la de benignas. Concluimos que este
atributo sí puede aportarnos información sobre la severidad del tumor.

\begin{figure}[H]
  \centering
  \caption{Distribución de ejemplos de cada clase para los distintos valores de Shape}
  \includegraphics[width=115mm]{figures/visualizacion/shape}
  \label{fig:shape}
\end{figure}

La mayoría de instancias que presentan el valor I de esta variable,
corresponden a tumores malignos. La mayoría de las instancias que
presentan los valores O o R de esta variable, corresponden a
benignos. Mientras que hay aproximadamente las mismas instancias
benignas que malignas entre las que tienen valor L. Por tanto, esta
variable también está relacionada con la severidad.

\begin{center}
\vspace*{8cm}
\part{\textbf{Segmentación}}
\end{center}

\setcounter{section}{0}
\renewcommand*{\theHsection}{\theHpart.\the\value{section}}

\section{Introducción}

En esta práctica emplearemos técnicas de aprendizaje no supervisado,
como es el clustering, para realizar un análisis relacional mediante
segmentación.

Nuestro dataset consta de todos los accidentes ocurridos en España
durante el año 2013. Los datos están publicados por la
\href{https://sedeapl.dgt.gob.es/WEB_IEST_CONSULTA/subcategoria.faces}{DGT}. Disponemos
de datos de 89519 accidentes, y 32 variables que miden circunstancias
en las que ocurre cada accidente (día, hora, visibilidad, superficie
de la calzada, factores atmosféricos, zona \ldots), el tipo (alcance,
choque frontal, choque lateral, atropello a peatón o animal, colisión
con obstáculo \ldots) y la gravedad (número de heridos leves y graves y
número de muertos).

Los algoritmos de clustering que utilizaremos son:
\begin{itemize}
\item \href{https://en.wikipedia.org/wiki/K-means_clustering}{\textbf{K-means:}} Un algoritmo iterativo basado en
  particionamiento. Es relativamente eficiente ($O(tkn)$ donde $n$ es
  el número de datos, $k$ el de clústers y $t$ el de iteraciones) y
  suele alcanzar óptimos locales bastante buenos. Por otra parte, sólo
  trabaja bien cuando los datos son numéricos, necesita prefijar el
  número de clusters, sólo encuentra clusters esféricos y no lidia
  bien con datos ruidosos y outliers.

\item
  \href{https://en.wikipedia.org/wiki/DBSCAN}{\textbf{DBSCAN:}}
  También basado en particionamiento. No necesita a priori el número
  de clusters, en su lugar requiere el radio máximo de cada clúster y
  el tamaño mínimo de los mismos. También es capaz de encontrar
  clusters con distintas formas y es robusto a datos ruidosos y
  outliers.

\item \href{https://en.wikipedia.org/wiki/Ward%27s_method}
    {\textbf{Ward:}} Es un método aglomerativo, es decir, parte de un
    clúster para cada elemento y fusiona los clusters que generen en
    el agrupamiento con mínima varianza. Necesita el número de clúster
    o una distancia límite a partir de la cual no fusiona dos
    clusters.
\end{itemize}

Para estimar los hiperparámetros de los algoritmos atenderemos a dos
métricas para interpretar la bondad de un agrupamiento:

\begin{itemize}
\item \href{https://en.wikipedia.org/wiki/Silhouette_%28clustering%29}
    {\textbf{Coeficiente Silhouette:}} Compara la similitud de de los
    objetos de un mismo cluster (cohesión) con los de otros clusters
    (separación). Para cada elemento $i$, se calcula $a(i)$ como la
    distancia media entre $i$ y el resto de elementos del cluster,
    mientras que que $b(i)$ representa el mínimo de las distancias
    entre $i$ y cada cluster distinto del suyo (la distancia de $i$ a
    un cluster se calcula como la media de las distancias de $i$ a
    cada elemento del cluster). Tomando
    \[s(i)=\frac{b(i)-a(i)}{\max{a(i),b(i)}}\] se tiene
    $s(i)\in[-1,1]$. Si $s(i)$ es negativo \big($b(i)<a(i)$\big),
    claramente el elemento $i$ pertenece al cluster equivocado (está
    más cerca de un cluster distinto al suyo); si $s(i)$ está cerca de
    1, significa que $i$ está mucho más cerca de los elementos de su
    cluster que del resto de clusters. El coeficiente silhoutte se
    calcula como la media de los $s(i)$ para todos los ejemplos
    $i$. Usualmente se encuentra entre 0 y 1.

  \item
    \href{https://www.tandfonline.com/doi/pdf/10.1080/03610927408827101?needAccess=true}{\textbf{Razón
        de Calinski-Harabasz:}} Razón entre la dispersión
    intra-clusters (dentro de cada cluster) cantidad que y la
    dispersión inter-clusters (entre clusters). Su cálculo es más
    complejo que el anterior y su valor no está entre 0 y 1, en
    nuestro caso toma valores del orden de miles.
  \end{itemize}

  Otro factor muy importante a tener en cuanta será el número de
  clusters generados, ya un elevado número de clusters dificulta las
  tareas de interpretación y análisis de los resultados. El hecho de
  generar un número excesivo de clusters con el fin de maximizar las
  métricas al precio de dificultar el análisis y la interpretabilidad
  sería el equivalente a lo que en aprendizaje supervisado conocíamos
  como sobreajuste, podemos llamarle también de ese modo.

  Debido al elevado número de datos, los algoritmos de clustering (que
  suelen presentar un elevado orden de complejidad) podrían tardar un
  tiempo excesivo en ejecutarse, por lo que realizaremos dos casos de
  estudio en cada uno de los cuales seleccionaremos un subconjunto de
  las instancias. Para cada caso de estudio, restringiremos el número
  de datos fijando los valores de una o más variables de carácter
  circunstancial o de tipo, y realizaremos el agrupamiento basándonos
  en las variables que miden la gravedad de los accidentes, pues son
  numéricas y nos evitan problemas como la dificultad de K-means para
  trabajar con variables nominales.

  Existe una amplia variedad de casos de estudio con sentido e interés
  que podríamos analizar: accidentes bajo condiciones climatológicas
  adversas, en zonas urbanas, atropellos, etc. Nosotros analizaremos
  los accidentes en condiciones óptimas para la conducción y los
  accidentes que ocurren a altas horas de la madrugada.

  Es importante normalizar los datos (reescalarlos por columnas para
  que estén en el intervalo $[0,1]$ antes de realizar la segmentación
  para que los algoritmos funcionen correctamente. De lo contrario,
  algunas dimensiones cobrarían más importancia que otras. Para ello,
  utilizamos
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html}{\texttt{MinMaxScaler}}
  de \textit{sklearn}.

  Para ciertas tareas como el cálulo de las métricas y la
  visualización de los centroides y las distribuciones de los
  clusters, utilizamos las funciones que se nos proporcionan en el
  fichero \texttt{pract2\_utils.py}, que internamente utiliza
  \textit{skleran} para las métricas y \textit{seaborn} para las
  visualizaciones. Hemos relizado pequeñas modificaciones en este
  fichero: la posibilidad de introducir los centroides ya
  desnormalizados para su visualización y cambios en las paletas de
  colores para hacer las visualizaciones más fácilmente interpretables.
  
\section{Caso de estudio 1: Análisis de los accidentes en condiciones óptimas para la conducción}

El primer caso de estudio seleccionaremos los accidentes que ocurren
en condiciones óptimas para la conducción. Estos accidentes pueden
ocurrir bien por factor vehículo o bien por factor humano, ya que
eliminaremos el factor vía seleccionando ejemplos de accidentes que
ocurren en calzadas secas y limpias. No disponemos de datos relativos
al factor vehículo, pero dentro del factor humano, eliminaremos
también las condiciones que favorecen los errores humanos como la
falta de luminosidad, visibilidad o la circulación densa. Teniendo en
cuenta que el factor vehículo es causa de un porcentaje relativamente
bajo de accidentes (menos del 10\%), principalmente tenemos accidentes
debidos al factor humano que han ocurrido en condiciones óptimas para
la conducción: buen tiempo, buena visibilidad, calzada limpia y seca,
tráfico fluido, etc. Es decir, accidentes debidos a factor humano que
se podrían haber evitado. Aquí radica el interés de este caso de
estudio.

En concreto, estos son los valores que fijamos para las variables:

\begin{verbatim}
LUMINOSIDAD=='NOCHE: ILUMINACIÓN SUFICIENTE' | LUMINOSIDAD=='PLENO DÍA'
DENSIDAD_CIRCULACION=='FLUIDA'
DIASEMANA==6
OTRA_CIRCUNSTANCIA=='NINGUNA'
FACTORES_ATMOSFERICOS=='BUEN TIEMPO'
MEDIDAS_ESPECIALES=='NINGUNA MEDIDA'
SUPERFICIE_CALZADA=='SECA Y LIMPIA'
VISIBILIDAD_RESTRINGIDA=='SIN RESTRICCIÓN'
\end{verbatim}

Con \texttt{OTRA\_CIRCUNSTANCIA=='NINGUNA'} descartamos accidentes
ocurridos con obras, inundaciones, baches, badenes, cambios de
rasante, estrechamientos, pasos a nivel y demás circunstancias que
podrían dificultar la conducción. Con
\texttt{MEDIDAS\_ESPECIALES=='NINGUNA MEDIDA'} descartamos accidentes
ocurridos en carriles reversibles o con habilitación del arcén, que
podríamos considerar fuera de la conducción ``normal''.

Además, para quedarnos con menos datos nos restringimos a los
accidentes ocurridos en sábado con \texttt{DIASEMANA==6}. Los días del
lunes al jueves son más propensos a presentar accidentes con una sola
víctima (como apreciamos en la Figura \ref{fig:diasemana}), esto ocurre
porque va mucha más gente a trabajar, mientras que en fin de semana es
más usual que viajen varias personas en el coche. Así que elegimos
este día para obtener un mayor número de accidentes con múltiples
víctimas. El hecho de que viajen más personas en el coche a parte del
conductor, por una parte puede aumentar la responsabilidad y por otra
puede aumentar las distracciones o crear una falsa sensación de
confianza. Esto hace los accidentes ocurridos en fin de semana más
interesantes, en mi opinión. Motivo por el cuál elegimos un día del
fin de semana.

\begin{figure}[H]
  \centering
  \includegraphics[width=140mm]{figures/accidentes/diasemana}
  \caption{De lunes a jueves la mayoría de accidentes ocasionan una
    sola víctima (considera los accidentes con más víctimas como
    outliers). En fin de semana hay más accidentes con múltiples
    víctimas.}
  \label{fig:diasemana}
\end{figure}
\vspace{-5mm}
Fijando estos valores, nos quedamos con 4143 instancias, por lo que
nuestros algoritmos se ejecutarán bastante rápido.

Anticipando problemas posteriores, debemos eliminar los outliers para
poder visualizar la segmentación correctamente. Utilizando
\texttt{Counter} observamos que hay un accidente con 52 víctimas, esto
distorsiona las escalas de los ejes en las gráficas de víctimas y
heridos, lo que dificulta su interpretación.

Por tanto, eliminamos los outliers con el siguiente código, que
colapsa los valores por encima del máximo de la whisker box, que
admite hasta la media más 3 por la desviación típica
($\mu+3\cdot\sigma$), en dicho máximo. 

\begin{verbatim}
# Eliminar outliers, ya que distorsionan las gráficas
for a in atributos:
    if a in {'TOT_MUERTOS','TOT_HERIDOS_GRAVES'}:
        continue
    d=data[a][abs(zscore(data[a]))<3]
    data[a][zscore(data[a])<-3]=d.min()
    data[a][zscore(data[a])>3]=d.max()
\end{verbatim}

La gran mayoría de accidentes no presenta muertos ni heridos graves,
lo que provoca que los muertos y heridos graves se consideren
outliers, por eso los dejamos tal y como están. En la siguiente
gráfica podemos comparar las distribuciones de las variables, se
elminan la mayoría de outliers y se regula la escala de los ejes,
salvo en los dos atributos que no hemos modificado.

\begin{figure}[H]
  \centering
  \subfigure[Antes de eliminar outliers.]{\includegraphics[width=160mm]{figures/accidentes/outliers1}}
  \subfigure[Tras eliminar outliers.]{\includegraphics[width=160mm]{figures/accidentes/outliers2}}
  \caption{Distribución de los valores de cada variable.}
  \label{fig:ward1-scores}
\end{figure}

\subsection{Algoritmos de clustering y resultados}

El primer algoritmo que probaremos para abordar este problema es
\textbf{K-means}, utilizaremos la
\href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html}{implementación}
del módulo de clusters de \textit{sklearn}. En primer lugar debemos
escoger un valor de $K$ adecuado, para ello nos basamos en las
métricas Silhouette y Calinski(-Harabasz). Probando diferentes valores
de $K$, obtenemos la siguiente tabla:

\begin{table}[H]
  \centering
\begin{tabular}{|c|cc|}
  \hline
  ~\hspace{2mm}K\hspace{2mm}~ & ~\hspace{2mm}Calinski\hspace{2mm}~ & ~\hspace{2mm}Silhouette\hspace{2mm}~ \\ \hline
2 & 3042.33 & 0.5541 \\ \hline
3 & 4703.28 & 0.6437 \\ \hline
4 & 4136.87 & 0.6577 \\ \hline
5 & 4038.97 & 0.6892 \\ \hline
6 & 3853.22 & 0.7027 \\ \hline
7 & 4151.42 & 0.7369 \\ \hline
8 & 4226.28 & 0.7867 \\ \hline
9 & 4805.5 & 0.8417 \\ \hline
\end{tabular}
\caption{Scores de K-means para distinto número de clusters.}
\label{tab:k-means1}
\end{table}

Nos ayudamos de representaciones gráficas para elegir un valor de $K$
adecuado. Buscamos un valor bajo para $K$ y valores altos para las
métricas.

\begin{figure}[H]
  \centering
  \subfigure[Razón de Calinski-Harabasz]{\includegraphics[width=87mm]{figures/accidentes/k-means1cal}}
  \subfigure[Coeficiente Silhouette]{\includegraphics[width=87mm]{figures/accidentes/k-means1sil}}
  \caption{Representaciones de los scores de K-means para distinto número de clusters.}
  \label{fig:k-means1-scores}
\end{figure}

La gráfica de Calinski claramente sugiere tomar $K=3$, ya que presenta
un máximo local bastante alto, sólo superado por $K=9$ y no por
mucho. En cambio, la gráfica de Silhouette no experimenta una ganancia
tan abrupta para este valor, pero la ganancia es aun más suave para
valores posteriores, por lo que de no tomar $K=3$, sugeriría tomar
$K=8$ o $K=9$. Esto complica bastante la posterior interpretación de
la segmentación, por lo que nos decantamos por $K=3$.

El siguiente código realiza el agrupamiento y obtiene las etiquetas
(el cluster al que pertenece cada ejemplo) y los centroides de cada
cluster.

\begin{verbatim}
K=3
results = KMeans(n_clusters=K, random_state=0).fit(data_norm)
labels=results.labels_
centroids=results.cluster_centers_
\end{verbatim}

Con \texttt{Counter}, consultamos el tamaño (número de elementos) de
cada cluster. Observamos que el cluster 0 es bastante más pequeño que
los otro dos.
\begin{verbatim}
Counter(labels)
> Counter({0: 419, 1: 1673, 2: 2051})
\end{verbatim}

Dejaremos la visualización de los centroides y de las distribuciones
de los clusters para la interpretación, en la Sección
\ref{sec:interpretacion1}. Relizaremos ahora el agrupamiento con el
algoritmo \textbf{Ward}, un método acumulativo. La implementación que
usamos es la de
\href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html}{clustering
  aglomerativo} de \textit{sklearn}, indicando Ward (mínima varianza)
como criterio para fusionar clusters (parámetro \texttt{linkage}).

Para seleccionar un número adecuado de clusters, realizamos un
experimento análogo al de la Tabla \ref{tab:k-means1}. Obteniendo:

\begin{table}[H]
  \centering
\begin{tabular}{|c|cc|}
  \hline
  ~\hspace{2mm}K\hspace{2mm}~ & ~\hspace{2mm}Calinski\hspace{2mm}~ & ~\hspace{2mm}Silhouette\hspace{2mm}~ \\ \hline
2 & 3042.33 & 0.5541 \\ \hline
3 & 4703.28 & 0.6437 \\ \hline
4 & 4136.87 & 0.6577 \\ \hline
5 & 4038.97 & 0.6892 \\ \hline
6 & 3853.22 & 0.7027 \\ \hline
7 & 4151.42 & 0.7369 \\ \hline
8 & 4226.28 & 0.7867 \\ \hline
9 & 4805.5 & 0.8417 \\ \hline
\end{tabular}
\caption{Scores de Ward para distinto número de clusters.}
\label{tab:ward1}
\end{table}

\begin{figure}[H]
  \centering
  \subfigure[Razón de Calinski-Harabasz]{\includegraphics[width=87mm]{figures/accidentes/ward1cal}}
  \subfigure[Coeficiente Silhouette]{\includegraphics[width=87mm]{figures/accidentes/ward1sil}}
  \caption{Representaciones de los scores de Ward para distinto número de clusters.}
  \label{fig:ward1-scores}
\end{figure}

La métrica de Calinski sugiere los valores $K=3$ y $K=6$, mientras que
Silhouette crece prácticamente de forma lineal. Tomaremos $K=6$, ya
que la ganancia en la métrica de Calinski es significativa y así
tendremos la oportunidad de analizar un agrupamiento con un número de
clusters algo mayor.

El siguiente código realiza el agrupamiento y obtiene las etiquetas
(el cluster al que pertenece cada ejemplo).

\begin{verbatim}
K=6
results = AgglomerativeClustering(distance_threshold=None, n_clusters=K).fit(data_norm)
labels=results.labels_
\end{verbatim}

Este algoritmo no devuelve los centroides, para calcularlos agrupamos
los datos por etiqueta y hacemos la media de cada atributo.

\begin{verbatim}
dataC=data.copy()
dataC['cluster']=labels
centroids = dataC.groupby('cluster').mean()
\end{verbatim}

Vemos que hay diferentes tamaños de cluster.

\begin{verbatim}
Counter(labels)
> Counter({0: 1493, 1: 549, 2: 1535, 3: 140, 4: 264, 5: 162})
\end{verbatim}

\subsection{Interpretación de la
  segmentación} \label{sec:interpretacion1}

\section{Caso de estudio 2: Análisis de los accidentes a altas horas de la madrugada}

\subsection{Algoritmos de clustering y resultados}

\href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html}{\textbf{DBSCAN:}}

\subsection{Interpretación de la segmentación}

\end{document}
